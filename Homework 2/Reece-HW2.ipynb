{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# <p style=\"text-align: center;\">MIS382: Advanced Predictive Modeling</p>\n",
    "# <p style=\"text-align: center;\">Assignment 2</p>\n",
    "## <p style=\"text-align: center;\">Total points: 40</p>\n",
    "## <p style=\"text-align: center;\">Due: Wed, October 4th, submitted via Canvas by 11:59 pm</p>\n",
    "\n",
    "Your homework should be written in a **Jupyter notebook**. You may work in groups of two if you wish. Only one student per team needs to submit the assignment on Canvas.  But be sure to include name and UTID for both students.  Homework groups will be created and managed through Canvas, so please do not arbitrarily change your homework group.  \n",
    "\n",
    "Also, please make sure your code runs and the graphics (and anything else) are displayed in your notebook before submitting. (%matplotlib inline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1 - Bias-variance Trandeoff (2pts)\n",
    "How does the choice of K in the K-nearest neighbor classifier reflect a bias-variance tradeoff?\n",
    "\n",
    "## Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When K=1 K-nearest neighbors will classify a point the same as its first nearest neighbor, this has the danger of overfitting the data causing the variance of the model to be high. This means that when the model is deployed on an unseen data set, there will likley be a lot of error. In contrast when K is high all the points will be classified as the largest class in the distribution, this will cause bias in the model because there is no information gained from K-nearest neighbors, and when the model is deployed on unseen data the accuracy rate will be the baseline accruacy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: Data Exploration and Regression Analysis (4+3+4+4=15pts)\n",
    "\n",
    "Consider the dataset provided (russett_full_v2.csv) about agricultural inequality, industrial development and political instability in different countries. More information about it can be found [here](https://www.rdocumentation.org/packages/plspm/versions/0.4.9/topics/russett) though the data itself is slightly different than that referenced in the link.\n",
    "\n",
    "a) (4 points) Generate box-plots of the \"rent\" (% of farmers that rent all their land), \"inst\" (measure of political stability in the executive branch), \"ecks\" (number of violent internal war incidents ) and \"demo_score\" ( derived measure of the level of a country's democracy from 1945 to 61 ) and identify the cutoff values for outliers. \n",
    "\n",
    "Generate 3 scatterplots of \"rent\" against \"demo_score\", \"inst\" against \"demo_score\" and \"ecks\" against \"demo_score\" with the identified outliers colored differently than non-outliers in each; comment on how inclusion of the outliers would affect a predictive model for the \"demo_score\" response.  \n",
    "\n",
    "b) (3 points) Let us try to fit an MLR, using ordinary least squares, to this dataset with \"demo_score\" as the dependent variable using only the predictors 'rent','inst', and 'ecks' . \n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.30, random_state=22)   \n",
    "\n",
    "Report the RMSE obtained on both X_train and X_test. How much does this increase when you score your model on X_test?\n",
    "\n",
    "c) (4 points ) Try to predict ”demo_score” using a robust regression using Huber loss.  You can use the [sklearn package](  http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.HuberRegressor.html).  Set regularization parameter alpha to 0.0 and all other parameters as default.\n",
    "Report RMSE obtained on both X_train and X_test.\n",
    "\n",
    "d) (4 points ) Compare and comment on the model fits obtained in (b) and (c) and plot the residual plots using all data for each model. How do outliers affect the relative performance of ordinary least squares regression, and robust regression with Huber loss in general? \n",
    "\n",
    "## Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) (4 points) Generate box-plots of the \"rent\" (% of farmers that rent all their land), \"inst\" (measure of political stability in the executive branch), \"ecks\" (number of violent internal war incidents ) and \"demo_score\" ( derived measure of the level of a country's democracy from 1945 to 61 ) and identify the cutoff values for outliers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "from pandas import Series, DataFrame\n",
    "import pandas as pd\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "File /Users/ReeceWooten/Documents/School /MSBA/Fall /Adv.Predictive/russett_full_v2.csv does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-9c972e8d0120>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/Users/ReeceWooten/Documents/School /MSBA/Fall /Adv.Predictive/russett_full_v2.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\deeks\\Anaconda2\\lib\\site-packages\\pandas\\io\\parsers.pyc\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    653\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    654\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 655\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    656\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\deeks\\Anaconda2\\lib\\site-packages\\pandas\\io\\parsers.pyc\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    403\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 405\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    406\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    407\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\deeks\\Anaconda2\\lib\\site-packages\\pandas\\io\\parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    762\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    763\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 764\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    765\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\deeks\\Anaconda2\\lib\\site-packages\\pandas\\io\\parsers.pyc\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m    983\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 985\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    986\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    987\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\deeks\\Anaconda2\\lib\\site-packages\\pandas\\io\\parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1603\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'allow_leading_cols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1604\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1605\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1606\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1607\u001b[0m         \u001b[1;31m# XXX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__ (pandas\\_libs\\parsers.c:4209)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source (pandas\\_libs\\parsers.c:8873)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mIOError\u001b[0m: File /Users/ReeceWooten/Documents/School /MSBA/Fall /Adv.Predictive/russett_full_v2.csv does not exist"
     ]
    }
   ],
   "source": [
    "data=pd.read_csv('/Users/ReeceWooten/Documents/School /MSBA/Fall /Adv.Predictive/russett_full_v2.csv')\n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "index_change=data[['country','rent','inst','ecks','demo_score']]\n",
    "index_change=index_change.set_index('country')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "index_change['rent'].plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_change['rent'].plot(kind='box')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "index_change['inst'].plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_change['inst'].plot(kind='box')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "index_change['ecks'].plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_change['ecks'].plot(kind='box')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_change['demo_score'].plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "index_change['demo_score'].plot(kind='box')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) (3 points) Let us try to fit an MLR, using ordinary least squares, to this dataset with \"demo_score\" as the dependent variable using only the predictors 'rent','inst', and 'ecks' .\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.30, random_state=22)   \n",
    "Report the RMSE obtained on both X_train and X_test. How much does this increase when you score your model on X_test?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Special packages\n",
    "import statsmodels.api as sm\n",
    "from patsy import dmatrices\n",
    "y, X = dmatrices('demo_score ~ rent+inst+ecks', data=data, return_type='dataframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.30, random_state=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(X_train, y_train)\n",
    "y_pred_train_ols = regr.predict(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rmse=sqrt(mean_squared_error(y_train, y_pred_train_ols))\n",
    "# The mean squared error\n",
    "print(\"Root Mean squared error on X_test: %.2f\"\n",
    "      % sqrt(mean_squared_error(y_train, y_pred_train_ols)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_test_ols = regr.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rmse=sqrt(mean_squared_error(y_test, y_pred_test_ols))\n",
    "# The mean squared error\n",
    "print(\"Root Mean squared error on X_test: %.2f\"\n",
    "      % sqrt(mean_squared_error(y_test, y_pred_test_ols)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rmse-train_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) (4 points ) Try to predict ”demo_score” using a robust regression using Huber loss. You can use the sklearn package. Set regularization parameter alpha to 0.0 and all other parameters as default. Report RMSE obtained on both X_train and X_test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import HuberRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train_array=array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "huber=HuberRegressor(alpha=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "huber.fit(X=X_train,y=y_train_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_test_huber = huber.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rmse=sqrt(mean_squared_error(y_test, y_pred_test_huber))\n",
    "# The mean squared error\n",
    "print(\"Root Mean squared error on X_test: %.2f\"\n",
    "      % sqrt(mean_squared_error(y_test, y_pred_test_huber)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_train_huber = huber.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rmse=sqrt(mean_squared_error(y_train, y_pred_train_huber))\n",
    "# The mean squared error\n",
    "print(\"Root Mean squared error on X_train: %.2f\"\n",
    "      % sqrt(mean_squared_error(y_train, y_pred_train_huber)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) (4 points ) Compare and comment on the model fits obtained in (b) and (c) and plot the residual plots using all data for each model. How do outliers affect the relative performance of ordinary least squares regression, and robust regression with Huber loss in general?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(X, y)\n",
    "y_pred_ols = regr.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_array=array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "y_=itertools.chain.from_iterable(y_array)\n",
    "y_array=array(list(y_))\n",
    "\n",
    "import itertools\n",
    "y_p=itertools.chain.from_iterable(y_pred_ols)\n",
    "y_pred_array=array(list(y_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "residual_ols=y_array-y_pred_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x112809d10>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEICAYAAABcVE8dAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZlYTDnwzs5pw4JuZ1YQD38ysJv4/ChZzC8gp3TUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x112a371d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scatter(y=residual_ols,x=y_array)\n",
    "title('y vs OLS residuals')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_array=array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "train=itertools.chain.from_iterable(y_array)\n",
    "y_array=array(list(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "huber=HuberRegressor(alpha=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuberRegressor(alpha=0.0, epsilon=1.35, fit_intercept=True, max_iter=100,\n",
       "        tol=1e-05, warm_start=False)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "huber.fit(X=X,y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_huber = huber.predict(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "residual_test=y_array-y_pred_huber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x11283fe50>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEICAYAAABcVE8dAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAxmuyy22JV5XAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x112977d10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scatter(y=residual_test,x=y_array)\n",
    "title('y vs Huber residuals')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3 - Sampling (4+4=8pts)\n",
    "\n",
    "1. NBC has come up with an extreme TV show, and each of its viewers either likes or hates it. (no middle ground here; we are in a “black and white age”). NBC wants to estimate what fraction p of its audience like the show by “randomly” calling n viewers and tallying their responses so as to estimate the true value of p to a fractional accuracy of within ±ε%, with a confidence of (1 − α) × 100%. For α = 0.10, ε = 0.03 (i.e. your answer will be $\\hat{p}$ ± 0.03), what is the minimum value of n needed if true value (i) p = 0.5, (ii) p=0.25? \n",
    "\n",
    "2. Suppose for a certain value of p and choice of ε, you calculate that you will need 1000 samples for α = 0.02. You now decide to obtain a more accurate answer by either (i) reducing α to 0.01, keeping the same ε or by (ii) reducing ε by a factor of 2 from the original value, and increasing α to 0.05. In each case how many samples would you need now?\n",
    "\n",
    "## Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. NBC has come up with an extreme TV show, and each of its viewers either likes or hates it. (no middle ground here; we are in a “black and white age”). NBC wants to estimate what fraction p of its audience like the show by “randomly” calling n viewers and tallying their responses so as to estimate the true value of p to a fractional accuracy of within ±ε%, with a confidence of (1 − α) × 100%. For α = 0.10, ε = 0.03 (i.e. your answer will be  p̂ p^  ± 0.03), what is the minimum value of n needed if true value (i) p = 0.5, (ii) p=0.25?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i. \n",
    "n = p(1-p) (za/2/ e) 2\n",
    "\n",
    ".5(.5)(1.645/.03)^2=752"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".25(.75)(1.645/.03)^2=564"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 Suppose for a certain value of p and choice of ε, you calculate that you will need 1000 samples for α = 0.02. You now decide to obtain a more accurate answer by either (i) reducing α to 0.01, keeping the same ε or by (ii) reducing ε by a factor of 2 from the original value, and increasing α to 0.05. In each case how many samples would you need now?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".5(.5)(3.3/.03)^2= 3025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".5(.5)(1.96/.015)^2= 4268.44"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4 - Principal Component Analysis (10 points)\n",
    "\n",
    "Import the diabetes dataset as in Q1 and add the interaction variables.\n",
    "You should have 65 variables and one target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "diabetes = datasets.load_diabetes()\n",
    "X = diabetes.data\n",
    "y = diabetes.target\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "X = PolynomialFeatures(2, include_bias=False).fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, use [StandardScaler](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) to center and scale each feature as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "stdScaler = StandardScaler()\n",
    "X = stdScaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.80050009,  1.06548848,  1.29708846, ..., -0.60582632,\n",
       "        -0.58432873, -0.5786918 ],\n",
       "       [-0.03956713, -0.93853666, -1.08218016, ...,  0.78127042,\n",
       "         2.18675392,  1.85055575],\n",
       "       [ 1.79330681,  1.06548848,  0.93453324, ..., -0.73183676,\n",
       "        -0.46890921, -0.47161867],\n",
       "       ..., \n",
       "       [ 0.87686984,  1.06548848, -0.33441002, ..., -0.02102345,\n",
       "        -0.7405101 , -0.59987616],\n",
       "       [-0.9560041 , -0.93853666,  0.82123474, ..., -0.09079363,\n",
       "        -0.91899961, -0.47161867],\n",
       "       [-0.9560041 , -0.93853666, -1.53537419, ..., -0.72871818,\n",
       "        -0.44336002, -0.6682648 ]])"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Perform PCA using the sklearn [PCA](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) package.  Create i) a scree plot depicting the proportion of variance and ii) a cumulative proportion of variance explained by the principal components of the data (X matrix).  Refer to Figure 10.4 of JW for an example.  You may use the output attribute *explained variance ratio*. (3pts)\n",
    "\n",
    "(b) How many principal components (N1, N2, N3) are required to explain cumulative variance of 30%, 60%, and 90%, respectively? (3pts)\n",
    "\n",
    "(c) Fit an ordinary least squares linear regression using N1, N2, and N3 number of principal components, respectively. (This is called Principal Components Regression) Use entire dataset, e.g. 442 rows. Evaluate the models using mean squared error (MSE). (4pts)\n",
    "\n",
    "## Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Perform PCA using the sklearn PCA package. Create i) a scree plot depicting the proportion of variance and ii) a cumulative proportion of variance explained by the principal components of the data (X matrix). Refer to Figure 10.4 of JW for an example. You may use the output attribute explained variance ratio. (3pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca=PCA(n_components=65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_components=array(range(1,66))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fit=pca.fit(X)\n",
    "variance=fit.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIoACQUREAgoEEREB4P8DzH/z7ow5E9IAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11cc93c90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "line1, = ax.plot(n_components, variance, linewidth=2,\n",
    "                 label='Dashes set retroactively')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def running_sum(a):\n",
    "  tot = 0\n",
    "  for item in a:\n",
    "    tot += item\n",
    "    yield tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tot=list(running_sum(variance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11cd91510>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "line1, = ax.plot(n_components, tot, linewidth=2,\n",
    "                 label='Dashes set retroactively')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) How many principal components (N1, N2, N3) are required to explain cumulative variance of 30%, 60%, and 90%, respectively? (3pts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "comp_tot=DataFrame(tot,n_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.165760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.256865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.336175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.400882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.452496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.497157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.536240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.574404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.607856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.638018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.666707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.693448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.714894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.734720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.753322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.770724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.787213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.801855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.815799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.828915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.841689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.853623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.864340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.874324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.884000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.893249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.902403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.910583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.918641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.926364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.963519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.968132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.972515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.976589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.980406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.983983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.987334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.990353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.992830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.994323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.995647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.996538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.997304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.997919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.998463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.998841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.999190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.999414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.999585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.999681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.999753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.999819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.999880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.999925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.999963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.999994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         tot\n",
       "1   0.165760\n",
       "2   0.256865\n",
       "3   0.336175\n",
       "4   0.400882\n",
       "5   0.452496\n",
       "6   0.497157\n",
       "7   0.536240\n",
       "8   0.574404\n",
       "9   0.607856\n",
       "10  0.638018\n",
       "11  0.666707\n",
       "12  0.693448\n",
       "13  0.714894\n",
       "14  0.734720\n",
       "15  0.753322\n",
       "16  0.770724\n",
       "17  0.787213\n",
       "18  0.801855\n",
       "19  0.815799\n",
       "20  0.828915\n",
       "21  0.841689\n",
       "22  0.853623\n",
       "23  0.864340\n",
       "24  0.874324\n",
       "25  0.884000\n",
       "26  0.893249\n",
       "27  0.902403\n",
       "28  0.910583\n",
       "29  0.918641\n",
       "30  0.926364\n",
       "..       ...\n",
       "36  0.963519\n",
       "37  0.968132\n",
       "38  0.972515\n",
       "39  0.976589\n",
       "40  0.980406\n",
       "41  0.983983\n",
       "42  0.987334\n",
       "43  0.990353\n",
       "44  0.992830\n",
       "45  0.994323\n",
       "46  0.995647\n",
       "47  0.996538\n",
       "48  0.997304\n",
       "49  0.997919\n",
       "50  0.998463\n",
       "51  0.998841\n",
       "52  0.999190\n",
       "53  0.999414\n",
       "54  0.999585\n",
       "55  0.999681\n",
       "56  0.999753\n",
       "57  0.999819\n",
       "58  0.999880\n",
       "59  0.999925\n",
       "60  0.999963\n",
       "61  0.999994\n",
       "62  1.000000\n",
       "63  1.000000\n",
       "64  1.000000\n",
       "65  1.000000\n",
       "\n",
       "[65 rows x 1 columns]"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp_tot.columns = ['tot']\n",
    "comp_tot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3, 9 and 27"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Fit an ordinary least squares linear regression using N1, N2, and N3 number of principal components, respectively. (This is called Principal Components Regression) Use entire dataset, e.g. 442 rows. Evaluate the models using mean squared error (MSE). (4pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resource:\n",
    "http://www.science.smith.edu/~jcrouser/SDS293/labs/2016/lab11/Lab%2011%20-%20PCR%20and%20PLS%20Regression%20in%20Python.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ReeceWooten/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn import cross_validation\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.cross_decomposition import PLSRegression, PLSSVD\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5120.6055477937271"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_reduced = fit.transform(X)[:,:3]\n",
    "# Train regression model on training data\n",
    "regr = LinearRegression()\n",
    "regr.fit(X_reduced, y)\n",
    "# Prediction with test data\n",
    "pred = regr.predict(X_reduced)\n",
    "mean_squared_error(y, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3942.7867514263485"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_reduced = fit.transform(X)[:,:9]\n",
    "# Train regression model on training data\n",
    "regr = LinearRegression()\n",
    "regr.fit(X_reduced, y)\n",
    "# Prediction with test data\n",
    "pred = regr.predict(X_reduced)\n",
    "mean_squared_error(y, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2857.425949000683"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_reduced = fit.transform(X)[:,:27]\n",
    "# Train regression model on training data\n",
    "regr = LinearRegression()\n",
    "regr.fit(X_reduced, y)\n",
    "# Prediction with test data\n",
    "pred = regr.predict(X_reduced)\n",
    "mean_squared_error(y, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5 -  Feature Selection ( 5 points )\n",
    "\n",
    "Explain what you understand by the two wrapper methods for feature selection (forward and backward selection) (no more than 1 paragraph). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Forward selection is the method of adding variables to the model one by one and only stopping when the newly added variables are no longer statistically significant. This method has draw backs because adding variables to the model one by one will sometimes cause previously significant variables to become insignificant. Backward Selection fixes this draw back by putting all the variables into the model and only taking out the ones which are insignificant, and rerunning the model until all variables are significant. Backward selection also has a drawback that a previously dropped variable might be significant in the final reduced model, but was taken out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
