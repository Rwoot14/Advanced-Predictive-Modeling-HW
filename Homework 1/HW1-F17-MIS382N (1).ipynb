{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# <p style=\"text-align: center;\">MIS 382N: ADVANCED PREDICTIVE MODELING - MSBA</p>\n",
    "# <p style=\"text-align: center;\">Assignment 1</p>\n",
    "## <p style=\"text-align: center;\">Total points: 75</p>\n",
    "## <p style=\"text-align: center;\">Due: Tuesday, September 13 submitted via Canvas by 11:59 p</p>\n",
    "\n",
    "Your homework should be written in a **Jupyter notebook**. You may work in groups of two if you wish. Only one student per team needs to submit the assignment on Canvas.  But be sure to include name and UTEID for both students.  Homework groups will be created and managed through Canvas, so please do not arbitrarily change your homework group.  \n",
    "\n",
    "Also, please make sure your code runs and the graphics (and anything else) are displayed in your notebook before submitting. (%matplotlib inline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: Applications of machine learning (10 pts)\n",
    "\n",
    "Read the [article](http://www.datasciencecentral.com/profiles/blogs/20-data-science-systems-used-by-amazon-to-operate-its-business) \"21 data science systems used by Amazon to operate its business\" and pick any two of the data science systems used by Amazon according to this blog.\n",
    "\n",
    "(5 pts each) For each of these two system you have chosen:\n",
    "\n",
    "What kind of machine learning problem is involved (e.g. classification, regression, clustering, outlier detection,...)? Speculate on what kind of data may be needed and how the results can be useful to the company.\n",
    "\n",
    "\n",
    "## Answer\n",
    "Supply chain optimization (I). Sites selection for warehouses to minimize distribution costs (proximity to vendors, balanced against proximity to consumers). How many warehouses are needed, and what capacity each of them should have.\n",
    "\n",
    "Fake reviews detection. They still have tons of progress to make in this area: at least categorizing users would be a first step, so that buyers know what kind of user produced a specific review; then relevancy algorithms must be used to assess how relevant a review is for a specific product, knowing that most likes and stars assigned by users are biased - partly because most normal people don't have time or interest to write a review. Indeed, fake reviews is a lucrative business taking advantages of inefficiencies in platforms such as Amazon. The best solution is to remove user-generated reviews and replace them, for each product, by number of sales over the last 30 days.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: Maximum likelihood estimate (10 pts)\n",
    "\n",
    "Suppose a manager at an internet sales company wants to estimate how fast his salesperson is generating successful leads. Instead of recording the time for each lead, the time taken to generate the next 5 leads are recorded, i.e., there is one recording (denoting the elapsed time) for every 5 consecutive leads. For a specific salesperson, the time intervals recorded are {1,3,1.5,4,2,7,1.2,2,4,3.1} hours. \n",
    "\n",
    "A statistician suggests that if these time intervals are assumed to arise by i.i.d. sampling from the following distribution:\n",
    "$$ p(t) = \\frac{1}{C \\times \\theta^{5}}t^{4}exp^{-\\frac{t}{\\theta}},$$\n",
    "(where C is a normalizing constant). Therefore, if $\\theta$ can be estimated, then he can provide detailed information\n",
    "about the lead generation process, including average rates, variances etc.\n",
    "\n",
    "Find the Maximum Likelihood estimate for $\\theta$ based on the recorded observations.\n",
    "\n",
    "\n",
    "## Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3: Multiple Linear Regression in Python (25 pts)\n",
    "\n",
    "Use the following code to import the boston housing dataset and linear models in python.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = datasets.load_boston()\n",
    "\n",
    "X = boston.data\n",
    "y = boston.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset information can be found [here](http://scikit-learn.org/stable/datasets/index.html#boston-house-prices-dataset).\n",
    "\n",
    "a. (3 pts) Print the shape (number of rows and columns) of the feature matrix, and print the first 5 rows.\n",
    "\n",
    "b.  (6 pts) Using ordinary least squares, fit a multiple linear regression (MLR) on all the feature variables using the entire dataset (506 rows). Report the regression coefficient of each input feature and evaluate the model using mean squared error (MSE).  Example of ordinary least squares in Python is shown in Section 1.1.1 of http://scikit-learn.org/stable/modules/linear_model.html.\n",
    "\n",
    "c.  (6 pts) Split the data into a training set and a test set.  Use the first 400 rows for training set and remaining rows for test set.  Fit an MLR using the training set.  Evaluate the trained model using the training set and the test set, respectively.  Compare the two MSE values thus obtained.\n",
    "\n",
    "d.  (6 pts) Do you think your MLR model is reasonable for this problem? You may look at the distribution of residuals to provide an informed answer.\n",
    "\n",
    "e. (5 pts) Use the following code to add new features to the dataset.  You should have 26 variables now.  Note that this code adds one squared term for each variable; in practice one may introduce only a few terms based on domain knowledge or experimentation.  Repeat (c) and report the MSE values of the training set and the test set, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate((X, np.square(X)), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ReeceWooten/anaconda/lib/python2.7/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "from pandas import Series, DataFrame\n",
    "import pandas as pd\n",
    "%pylab inline\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "# Special packages\n",
    "import statsmodels.api as sm\n",
    "from patsy import dmatrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.02985</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.430</td>\n",
       "      <td>58.7</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.12</td>\n",
       "      <td>5.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.08829</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.012</td>\n",
       "      <td>66.6</td>\n",
       "      <td>5.5605</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>395.60</td>\n",
       "      <td>12.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.14455</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.172</td>\n",
       "      <td>96.1</td>\n",
       "      <td>5.9505</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>19.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.21124</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>5.631</td>\n",
       "      <td>100.0</td>\n",
       "      <td>6.0821</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>386.63</td>\n",
       "      <td>29.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.17004</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.004</td>\n",
       "      <td>85.9</td>\n",
       "      <td>6.5921</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>386.71</td>\n",
       "      <td>17.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.22489</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.377</td>\n",
       "      <td>94.3</td>\n",
       "      <td>6.3467</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>392.52</td>\n",
       "      <td>20.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.11747</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.009</td>\n",
       "      <td>82.9</td>\n",
       "      <td>6.2267</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>13.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.09378</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>5.889</td>\n",
       "      <td>39.0</td>\n",
       "      <td>5.4509</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>390.50</td>\n",
       "      <td>15.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.62976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.949</td>\n",
       "      <td>61.8</td>\n",
       "      <td>4.7075</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>8.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.63796</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.096</td>\n",
       "      <td>84.5</td>\n",
       "      <td>4.4619</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>380.02</td>\n",
       "      <td>10.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.62739</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.834</td>\n",
       "      <td>56.5</td>\n",
       "      <td>4.4986</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>395.62</td>\n",
       "      <td>8.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.05393</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.935</td>\n",
       "      <td>29.3</td>\n",
       "      <td>4.4986</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>386.85</td>\n",
       "      <td>6.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.78420</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.990</td>\n",
       "      <td>81.7</td>\n",
       "      <td>4.2579</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>386.75</td>\n",
       "      <td>14.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.80271</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.456</td>\n",
       "      <td>36.6</td>\n",
       "      <td>3.7965</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>288.99</td>\n",
       "      <td>11.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.72580</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.727</td>\n",
       "      <td>69.5</td>\n",
       "      <td>3.7965</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>390.95</td>\n",
       "      <td>11.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.25179</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.570</td>\n",
       "      <td>98.1</td>\n",
       "      <td>3.7979</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>376.57</td>\n",
       "      <td>21.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.85204</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.965</td>\n",
       "      <td>89.2</td>\n",
       "      <td>4.0123</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>392.53</td>\n",
       "      <td>13.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.23247</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.142</td>\n",
       "      <td>91.7</td>\n",
       "      <td>3.9769</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>18.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.98843</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.813</td>\n",
       "      <td>100.0</td>\n",
       "      <td>4.0952</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>394.54</td>\n",
       "      <td>19.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.75026</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.924</td>\n",
       "      <td>94.1</td>\n",
       "      <td>4.3996</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>394.33</td>\n",
       "      <td>16.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.84054</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.599</td>\n",
       "      <td>85.7</td>\n",
       "      <td>4.4546</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>303.42</td>\n",
       "      <td>16.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.67191</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.813</td>\n",
       "      <td>90.3</td>\n",
       "      <td>4.6820</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>376.88</td>\n",
       "      <td>14.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.95577</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.047</td>\n",
       "      <td>88.8</td>\n",
       "      <td>4.4534</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>306.38</td>\n",
       "      <td>17.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.77299</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.495</td>\n",
       "      <td>94.4</td>\n",
       "      <td>4.4547</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>387.94</td>\n",
       "      <td>12.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.00245</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.674</td>\n",
       "      <td>87.3</td>\n",
       "      <td>4.2390</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>380.23</td>\n",
       "      <td>11.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>4.87141</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.614</td>\n",
       "      <td>6.484</td>\n",
       "      <td>93.6</td>\n",
       "      <td>2.3053</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>396.21</td>\n",
       "      <td>18.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>15.02340</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.614</td>\n",
       "      <td>5.304</td>\n",
       "      <td>97.3</td>\n",
       "      <td>2.1007</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>349.48</td>\n",
       "      <td>24.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>10.23300</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.614</td>\n",
       "      <td>6.185</td>\n",
       "      <td>96.7</td>\n",
       "      <td>2.1705</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>379.70</td>\n",
       "      <td>18.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>14.33370</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.614</td>\n",
       "      <td>6.229</td>\n",
       "      <td>88.0</td>\n",
       "      <td>1.9512</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>383.32</td>\n",
       "      <td>13.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>5.82401</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.532</td>\n",
       "      <td>6.242</td>\n",
       "      <td>64.7</td>\n",
       "      <td>3.4242</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>10.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>5.70818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.532</td>\n",
       "      <td>6.750</td>\n",
       "      <td>74.9</td>\n",
       "      <td>3.3317</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>393.07</td>\n",
       "      <td>7.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>5.73116</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.532</td>\n",
       "      <td>7.061</td>\n",
       "      <td>77.0</td>\n",
       "      <td>3.4106</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>395.28</td>\n",
       "      <td>7.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>2.81838</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.532</td>\n",
       "      <td>5.762</td>\n",
       "      <td>40.3</td>\n",
       "      <td>4.0983</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>392.92</td>\n",
       "      <td>10.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>2.37857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.583</td>\n",
       "      <td>5.871</td>\n",
       "      <td>41.9</td>\n",
       "      <td>3.7240</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>370.73</td>\n",
       "      <td>13.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>3.67367</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.583</td>\n",
       "      <td>6.312</td>\n",
       "      <td>51.9</td>\n",
       "      <td>3.9917</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>388.62</td>\n",
       "      <td>10.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>5.69175</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.583</td>\n",
       "      <td>6.114</td>\n",
       "      <td>79.8</td>\n",
       "      <td>3.5459</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>392.68</td>\n",
       "      <td>14.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>4.83567</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.583</td>\n",
       "      <td>5.905</td>\n",
       "      <td>53.2</td>\n",
       "      <td>3.1523</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>388.22</td>\n",
       "      <td>11.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>0.15086</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.74</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.609</td>\n",
       "      <td>5.454</td>\n",
       "      <td>92.7</td>\n",
       "      <td>1.8209</td>\n",
       "      <td>4.0</td>\n",
       "      <td>711.0</td>\n",
       "      <td>20.1</td>\n",
       "      <td>395.09</td>\n",
       "      <td>18.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>0.18337</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.74</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.609</td>\n",
       "      <td>5.414</td>\n",
       "      <td>98.3</td>\n",
       "      <td>1.7554</td>\n",
       "      <td>4.0</td>\n",
       "      <td>711.0</td>\n",
       "      <td>20.1</td>\n",
       "      <td>344.05</td>\n",
       "      <td>23.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>0.20746</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.74</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.609</td>\n",
       "      <td>5.093</td>\n",
       "      <td>98.0</td>\n",
       "      <td>1.8226</td>\n",
       "      <td>4.0</td>\n",
       "      <td>711.0</td>\n",
       "      <td>20.1</td>\n",
       "      <td>318.43</td>\n",
       "      <td>29.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>0.10574</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.74</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.609</td>\n",
       "      <td>5.983</td>\n",
       "      <td>98.8</td>\n",
       "      <td>1.8681</td>\n",
       "      <td>4.0</td>\n",
       "      <td>711.0</td>\n",
       "      <td>20.1</td>\n",
       "      <td>390.11</td>\n",
       "      <td>18.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>0.11132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.74</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.609</td>\n",
       "      <td>5.983</td>\n",
       "      <td>83.5</td>\n",
       "      <td>2.1099</td>\n",
       "      <td>4.0</td>\n",
       "      <td>711.0</td>\n",
       "      <td>20.1</td>\n",
       "      <td>396.90</td>\n",
       "      <td>13.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>0.17331</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.585</td>\n",
       "      <td>5.707</td>\n",
       "      <td>54.0</td>\n",
       "      <td>2.3817</td>\n",
       "      <td>6.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>12.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>0.27957</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.585</td>\n",
       "      <td>5.926</td>\n",
       "      <td>42.6</td>\n",
       "      <td>2.3817</td>\n",
       "      <td>6.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>13.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>0.17899</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.585</td>\n",
       "      <td>5.670</td>\n",
       "      <td>28.8</td>\n",
       "      <td>2.7986</td>\n",
       "      <td>6.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>393.29</td>\n",
       "      <td>17.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>0.28960</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.585</td>\n",
       "      <td>5.390</td>\n",
       "      <td>72.9</td>\n",
       "      <td>2.7986</td>\n",
       "      <td>6.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>21.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>0.26838</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.585</td>\n",
       "      <td>5.794</td>\n",
       "      <td>70.6</td>\n",
       "      <td>2.8927</td>\n",
       "      <td>6.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>14.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>0.23912</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.585</td>\n",
       "      <td>6.019</td>\n",
       "      <td>65.3</td>\n",
       "      <td>2.4091</td>\n",
       "      <td>6.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>12.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>0.17783</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.585</td>\n",
       "      <td>5.569</td>\n",
       "      <td>73.5</td>\n",
       "      <td>2.3999</td>\n",
       "      <td>6.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>395.77</td>\n",
       "      <td>15.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>0.22438</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.585</td>\n",
       "      <td>6.027</td>\n",
       "      <td>79.7</td>\n",
       "      <td>2.4982</td>\n",
       "      <td>6.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>14.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>0.06263</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.593</td>\n",
       "      <td>69.1</td>\n",
       "      <td>2.4786</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>391.99</td>\n",
       "      <td>9.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>0.04527</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.120</td>\n",
       "      <td>76.7</td>\n",
       "      <td>2.2875</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>0.06076</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.976</td>\n",
       "      <td>91.0</td>\n",
       "      <td>2.1675</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>0.10959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.794</td>\n",
       "      <td>89.3</td>\n",
       "      <td>2.3889</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>393.45</td>\n",
       "      <td>6.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>0.04741</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.030</td>\n",
       "      <td>80.8</td>\n",
       "      <td>2.5050</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>7.88</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>506 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0     1      2    3      4      5      6       7     8      9   \\\n",
       "0     0.00632  18.0   2.31  0.0  0.538  6.575   65.2  4.0900   1.0  296.0   \n",
       "1     0.02731   0.0   7.07  0.0  0.469  6.421   78.9  4.9671   2.0  242.0   \n",
       "2     0.02729   0.0   7.07  0.0  0.469  7.185   61.1  4.9671   2.0  242.0   \n",
       "3     0.03237   0.0   2.18  0.0  0.458  6.998   45.8  6.0622   3.0  222.0   \n",
       "4     0.06905   0.0   2.18  0.0  0.458  7.147   54.2  6.0622   3.0  222.0   \n",
       "5     0.02985   0.0   2.18  0.0  0.458  6.430   58.7  6.0622   3.0  222.0   \n",
       "6     0.08829  12.5   7.87  0.0  0.524  6.012   66.6  5.5605   5.0  311.0   \n",
       "7     0.14455  12.5   7.87  0.0  0.524  6.172   96.1  5.9505   5.0  311.0   \n",
       "8     0.21124  12.5   7.87  0.0  0.524  5.631  100.0  6.0821   5.0  311.0   \n",
       "9     0.17004  12.5   7.87  0.0  0.524  6.004   85.9  6.5921   5.0  311.0   \n",
       "10    0.22489  12.5   7.87  0.0  0.524  6.377   94.3  6.3467   5.0  311.0   \n",
       "11    0.11747  12.5   7.87  0.0  0.524  6.009   82.9  6.2267   5.0  311.0   \n",
       "12    0.09378  12.5   7.87  0.0  0.524  5.889   39.0  5.4509   5.0  311.0   \n",
       "13    0.62976   0.0   8.14  0.0  0.538  5.949   61.8  4.7075   4.0  307.0   \n",
       "14    0.63796   0.0   8.14  0.0  0.538  6.096   84.5  4.4619   4.0  307.0   \n",
       "15    0.62739   0.0   8.14  0.0  0.538  5.834   56.5  4.4986   4.0  307.0   \n",
       "16    1.05393   0.0   8.14  0.0  0.538  5.935   29.3  4.4986   4.0  307.0   \n",
       "17    0.78420   0.0   8.14  0.0  0.538  5.990   81.7  4.2579   4.0  307.0   \n",
       "18    0.80271   0.0   8.14  0.0  0.538  5.456   36.6  3.7965   4.0  307.0   \n",
       "19    0.72580   0.0   8.14  0.0  0.538  5.727   69.5  3.7965   4.0  307.0   \n",
       "20    1.25179   0.0   8.14  0.0  0.538  5.570   98.1  3.7979   4.0  307.0   \n",
       "21    0.85204   0.0   8.14  0.0  0.538  5.965   89.2  4.0123   4.0  307.0   \n",
       "22    1.23247   0.0   8.14  0.0  0.538  6.142   91.7  3.9769   4.0  307.0   \n",
       "23    0.98843   0.0   8.14  0.0  0.538  5.813  100.0  4.0952   4.0  307.0   \n",
       "24    0.75026   0.0   8.14  0.0  0.538  5.924   94.1  4.3996   4.0  307.0   \n",
       "25    0.84054   0.0   8.14  0.0  0.538  5.599   85.7  4.4546   4.0  307.0   \n",
       "26    0.67191   0.0   8.14  0.0  0.538  5.813   90.3  4.6820   4.0  307.0   \n",
       "27    0.95577   0.0   8.14  0.0  0.538  6.047   88.8  4.4534   4.0  307.0   \n",
       "28    0.77299   0.0   8.14  0.0  0.538  6.495   94.4  4.4547   4.0  307.0   \n",
       "29    1.00245   0.0   8.14  0.0  0.538  6.674   87.3  4.2390   4.0  307.0   \n",
       "..        ...   ...    ...  ...    ...    ...    ...     ...   ...    ...   \n",
       "476   4.87141   0.0  18.10  0.0  0.614  6.484   93.6  2.3053  24.0  666.0   \n",
       "477  15.02340   0.0  18.10  0.0  0.614  5.304   97.3  2.1007  24.0  666.0   \n",
       "478  10.23300   0.0  18.10  0.0  0.614  6.185   96.7  2.1705  24.0  666.0   \n",
       "479  14.33370   0.0  18.10  0.0  0.614  6.229   88.0  1.9512  24.0  666.0   \n",
       "480   5.82401   0.0  18.10  0.0  0.532  6.242   64.7  3.4242  24.0  666.0   \n",
       "481   5.70818   0.0  18.10  0.0  0.532  6.750   74.9  3.3317  24.0  666.0   \n",
       "482   5.73116   0.0  18.10  0.0  0.532  7.061   77.0  3.4106  24.0  666.0   \n",
       "483   2.81838   0.0  18.10  0.0  0.532  5.762   40.3  4.0983  24.0  666.0   \n",
       "484   2.37857   0.0  18.10  0.0  0.583  5.871   41.9  3.7240  24.0  666.0   \n",
       "485   3.67367   0.0  18.10  0.0  0.583  6.312   51.9  3.9917  24.0  666.0   \n",
       "486   5.69175   0.0  18.10  0.0  0.583  6.114   79.8  3.5459  24.0  666.0   \n",
       "487   4.83567   0.0  18.10  0.0  0.583  5.905   53.2  3.1523  24.0  666.0   \n",
       "488   0.15086   0.0  27.74  0.0  0.609  5.454   92.7  1.8209   4.0  711.0   \n",
       "489   0.18337   0.0  27.74  0.0  0.609  5.414   98.3  1.7554   4.0  711.0   \n",
       "490   0.20746   0.0  27.74  0.0  0.609  5.093   98.0  1.8226   4.0  711.0   \n",
       "491   0.10574   0.0  27.74  0.0  0.609  5.983   98.8  1.8681   4.0  711.0   \n",
       "492   0.11132   0.0  27.74  0.0  0.609  5.983   83.5  2.1099   4.0  711.0   \n",
       "493   0.17331   0.0   9.69  0.0  0.585  5.707   54.0  2.3817   6.0  391.0   \n",
       "494   0.27957   0.0   9.69  0.0  0.585  5.926   42.6  2.3817   6.0  391.0   \n",
       "495   0.17899   0.0   9.69  0.0  0.585  5.670   28.8  2.7986   6.0  391.0   \n",
       "496   0.28960   0.0   9.69  0.0  0.585  5.390   72.9  2.7986   6.0  391.0   \n",
       "497   0.26838   0.0   9.69  0.0  0.585  5.794   70.6  2.8927   6.0  391.0   \n",
       "498   0.23912   0.0   9.69  0.0  0.585  6.019   65.3  2.4091   6.0  391.0   \n",
       "499   0.17783   0.0   9.69  0.0  0.585  5.569   73.5  2.3999   6.0  391.0   \n",
       "500   0.22438   0.0   9.69  0.0  0.585  6.027   79.7  2.4982   6.0  391.0   \n",
       "501   0.06263   0.0  11.93  0.0  0.573  6.593   69.1  2.4786   1.0  273.0   \n",
       "502   0.04527   0.0  11.93  0.0  0.573  6.120   76.7  2.2875   1.0  273.0   \n",
       "503   0.06076   0.0  11.93  0.0  0.573  6.976   91.0  2.1675   1.0  273.0   \n",
       "504   0.10959   0.0  11.93  0.0  0.573  6.794   89.3  2.3889   1.0  273.0   \n",
       "505   0.04741   0.0  11.93  0.0  0.573  6.030   80.8  2.5050   1.0  273.0   \n",
       "\n",
       "       10      11     12  \n",
       "0    15.3  396.90   4.98  \n",
       "1    17.8  396.90   9.14  \n",
       "2    17.8  392.83   4.03  \n",
       "3    18.7  394.63   2.94  \n",
       "4    18.7  396.90   5.33  \n",
       "5    18.7  394.12   5.21  \n",
       "6    15.2  395.60  12.43  \n",
       "7    15.2  396.90  19.15  \n",
       "8    15.2  386.63  29.93  \n",
       "9    15.2  386.71  17.10  \n",
       "10   15.2  392.52  20.45  \n",
       "11   15.2  396.90  13.27  \n",
       "12   15.2  390.50  15.71  \n",
       "13   21.0  396.90   8.26  \n",
       "14   21.0  380.02  10.26  \n",
       "15   21.0  395.62   8.47  \n",
       "16   21.0  386.85   6.58  \n",
       "17   21.0  386.75  14.67  \n",
       "18   21.0  288.99  11.69  \n",
       "19   21.0  390.95  11.28  \n",
       "20   21.0  376.57  21.02  \n",
       "21   21.0  392.53  13.83  \n",
       "22   21.0  396.90  18.72  \n",
       "23   21.0  394.54  19.88  \n",
       "24   21.0  394.33  16.30  \n",
       "25   21.0  303.42  16.51  \n",
       "26   21.0  376.88  14.81  \n",
       "27   21.0  306.38  17.28  \n",
       "28   21.0  387.94  12.80  \n",
       "29   21.0  380.23  11.98  \n",
       "..    ...     ...    ...  \n",
       "476  20.2  396.21  18.68  \n",
       "477  20.2  349.48  24.91  \n",
       "478  20.2  379.70  18.03  \n",
       "479  20.2  383.32  13.11  \n",
       "480  20.2  396.90  10.74  \n",
       "481  20.2  393.07   7.74  \n",
       "482  20.2  395.28   7.01  \n",
       "483  20.2  392.92  10.42  \n",
       "484  20.2  370.73  13.34  \n",
       "485  20.2  388.62  10.58  \n",
       "486  20.2  392.68  14.98  \n",
       "487  20.2  388.22  11.45  \n",
       "488  20.1  395.09  18.06  \n",
       "489  20.1  344.05  23.97  \n",
       "490  20.1  318.43  29.68  \n",
       "491  20.1  390.11  18.07  \n",
       "492  20.1  396.90  13.35  \n",
       "493  19.2  396.90  12.01  \n",
       "494  19.2  396.90  13.59  \n",
       "495  19.2  393.29  17.60  \n",
       "496  19.2  396.90  21.14  \n",
       "497  19.2  396.90  14.10  \n",
       "498  19.2  396.90  12.92  \n",
       "499  19.2  395.77  15.10  \n",
       "500  19.2  396.90  14.33  \n",
       "501  21.0  391.99   9.67  \n",
       "502  21.0  396.90   9.08  \n",
       "503  21.0  396.90   5.64  \n",
       "504  21.0  393.45   6.48  \n",
       "505  21.0  396.90   7.88  \n",
       "\n",
       "[506 rows x 13 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DataFrame(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.959\n",
      "Model:                            OLS   Adj. R-squared:                  0.958\n",
      "Method:                 Least Squares   F-statistic:                     891.1\n",
      "Date:                Fri, 08 Sep 2017   Prob (F-statistic):               0.00\n",
      "Time:                        19:47:28   Log-Likelihood:                -1523.8\n",
      "No. Observations:                 506   AIC:                             3074.\n",
      "Df Residuals:                     493   BIC:                             3129.\n",
      "Df Model:                          13                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "x1            -0.0916      0.034     -2.675      0.008      -0.159      -0.024\n",
      "x2             0.0487      0.014      3.379      0.001       0.020       0.077\n",
      "x3            -0.0038      0.064     -0.059      0.953      -0.130       0.123\n",
      "x4             2.8564      0.904      3.160      0.002       1.080       4.633\n",
      "x5            -2.8808      3.359     -0.858      0.392      -9.481       3.720\n",
      "x6             5.9252      0.309     19.168      0.000       5.318       6.533\n",
      "x7            -0.0072      0.014     -0.523      0.601      -0.034       0.020\n",
      "x8            -0.9680      0.196     -4.947      0.000      -1.352      -0.584\n",
      "x9             0.1704      0.067      2.554      0.011       0.039       0.302\n",
      "x10           -0.0094      0.004     -2.393      0.017      -0.017      -0.002\n",
      "x11           -0.3924      0.110     -3.571      0.000      -0.608      -0.177\n",
      "x12            0.0150      0.003      5.561      0.000       0.010       0.020\n",
      "x13           -0.4170      0.051     -8.214      0.000      -0.517      -0.317\n",
      "==============================================================================\n",
      "Omnibus:                      204.050   Durbin-Watson:                   0.999\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1372.527\n",
      "Skew:                           1.609   Prob(JB):                    9.11e-299\n",
      "Kurtosis:                      10.399   Cond. No.                     8.50e+03\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 8.5e+03. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "model = sm.OLS(y, X)       # Set up the model\n",
    "result = model.fit()       # Fit model (find the intercept and slopes)\n",
    "print result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Coefficients: \\n', array([ -1.07170557e-01,   4.63952195e-02,   2.08602395e-02,\n",
      "         2.68856140e+00,  -1.77957587e+01,   3.80475246e+00,\n",
      "         7.51061703e-04,  -1.47575880e+00,   3.05655038e-01,\n",
      "        -1.23293463e-02,  -9.53463555e-01,   9.39251272e-03,\n",
      "        -5.25466633e-01]))\n",
      "Mean squared error: 21.90\n",
      "Variance score: 0.74\n"
     ]
    }
   ],
   "source": [
    "# Create linear regression object\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "# Train the model using the training sets\n",
    "regr.fit(X,y)\n",
    "\n",
    "# Make predictions using the testing set\n",
    "y_pred = regr.predict(X)\n",
    "\n",
    "# The coefficients\n",
    "print('Coefficients: \\n', regr.coef_)\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % mean_squared_error(y,y_pred))\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Variance score: %.2f' % r2_score(y,y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=X[1:400]\n",
    "y_train=y[1:400]\n",
    "X_test=X[401:506]\n",
    "y_test=y[401:506]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Coefficients: \\n', array([ -1.89604895e-01,   4.40445864e-02,   4.94266931e-02,\n",
      "         1.71077793e+00,  -1.48021604e+01,   4.86449168e+00,\n",
      "         3.00194986e-03,  -1.29848158e+00,   4.78222460e-01,\n",
      "        -1.51345166e-02,  -8.13314230e-01,  -1.01675810e-03,\n",
      "        -5.21506401e-01]))\n",
      "Mean squared error: 37.37\n",
      "Variance score: -0.36\n"
     ]
    }
   ],
   "source": [
    "# Create linear regression object\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "# Train the model using the training sets\n",
    "regr.fit(X_train,y_train)\n",
    "\n",
    "# Make predictions using the testing set\n",
    "y_pred = regr.predict(X_test)\n",
    "\n",
    "# The coefficients\n",
    "print('Coefficients: \\n', regr.coef_)\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % mean_squared_error(y_test,y_pred))\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Variance score: %.2f' % r2_score(y_test,y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Coefficients: \\n', array([ -1.89604895e-01,   4.40445864e-02,   4.94266931e-02,\n",
      "         1.71077793e+00,  -1.48021604e+01,   4.86449168e+00,\n",
      "         3.00194986e-03,  -1.29848158e+00,   4.78222460e-01,\n",
      "        -1.51345166e-02,  -8.13314230e-01,  -1.01675810e-03,\n",
      "        -5.21506401e-01]))\n",
      "Mean squared error: 22.30\n",
      "Variance score: 0.73\n"
     ]
    }
   ],
   "source": [
    "# Create linear regression object\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "# Train the model using the training sets\n",
    "regr.fit(X_train,y_train)\n",
    "\n",
    "# Make predictions using the testing set\n",
    "y_pred = regr.predict(X_train)\n",
    "\n",
    "# The coefficients\n",
    "print('Coefficients: \\n', regr.coef_)\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % mean_squared_error(y_train,y_pred))\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Variance score: %.2f' % r2_score(y_train,y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must be the same size",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-469d051720ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Plot outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'black'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/ReeceWooten/anaconda/lib/python2.7/site-packages/matplotlib/pyplot.pyc\u001b[0m in \u001b[0;36mscatter\u001b[0;34m(x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, verts, edgecolors, hold, data, **kwargs)\u001b[0m\n\u001b[1;32m   3432\u001b[0m                          \u001b[0mvmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvmax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3433\u001b[0m                          \u001b[0mlinewidths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlinewidths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3434\u001b[0;31m                          edgecolors=edgecolors, data=data, **kwargs)\n\u001b[0m\u001b[1;32m   3435\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3436\u001b[0m         \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwashold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ReeceWooten/anaconda/lib/python2.7/site-packages/matplotlib/__init__.pyc\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1896\u001b[0m                     warnings.warn(msg % (label_namer, func.__name__),\n\u001b[1;32m   1897\u001b[0m                                   RuntimeWarning, stacklevel=2)\n\u001b[0;32m-> 1898\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1899\u001b[0m         \u001b[0mpre_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1900\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpre_doc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ReeceWooten/anaconda/lib/python2.7/site-packages/matplotlib/axes/_axes.pyc\u001b[0m in \u001b[0;36mscatter\u001b[0;34m(self, x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, verts, edgecolors, **kwargs)\u001b[0m\n\u001b[1;32m   3962\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3963\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3964\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"x and y must be the same size\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3965\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3966\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must be the same size"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAADQdJREFUeJzt3F+IpfV9x/H3p7sRGpNGiZOQ7irZljVmobHoxEiR1jS0\n7tqLJeCFGiKVwCKNIZdKocmFN81FIQT/LIsskpvsRSPJppjYQkksWNOdBf+tokxXqquCq4YUDFQG\nv72Y087pdNd5duaZmXW+7xcMzHOe38z57o/Z9z57zpyTqkKStPX91mYPIEnaGAZfkpow+JLUhMGX\npCYMviQ1YfAlqYkVg5/kcJI3kjx7lvNJ8r0k80meTnLV+GNKktZqyBX+Q8De9zm/D9g9+TgAPLD2\nsSRJY1sx+FX1GPD2+yzZD3y/Fj0BXJTkU2MNKEkax/YRvscO4JWp41OT215fvjDJARb/F8CFF154\n9RVXXDHC3UtSH8ePH3+zqmZW87VjBH+wqjoEHAKYnZ2tubm5jbx7SfrAS/Ifq/3aMX5L51Xg0qnj\nnZPbJEnnkTGCfxS4bfLbOtcCv66q//dwjiRpc634kE6SHwDXA5ckOQV8G/gQQFUdBB4BbgTmgd8A\nt6/XsJKk1Vsx+FV1ywrnC/j6aBNJktaFr7SVpCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmjD4\nktSEwZekJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNWHwJakJgy9JTRh8\nSWrC4EtSEwZfkpow+JLUhMGXpCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmjD4ktSEwZekJgy+\nJDVh8CWpiUHBT7I3yQtJ5pPcfYbzH0vykyRPJTmR5PbxR5UkrcWKwU+yDbgP2AfsAW5JsmfZsq8D\nz1XVlcD1wN8luWDkWSVJazDkCv8aYL6qTlbVu8ARYP+yNQV8NEmAjwBvAwujTipJWpMhwd8BvDJ1\nfGpy27R7gc8CrwHPAN+sqveWf6MkB5LMJZk7ffr0KkeWJK3GWE/a3gA8Cfwu8IfAvUl+Z/miqjpU\nVbNVNTszMzPSXUuShhgS/FeBS6eOd05um3Y78HAtmgdeAq4YZ0RJ0hiGBP8YsDvJrskTsTcDR5et\neRn4EkCSTwKfAU6OOagkaW22r7SgqhaS3Ak8CmwDDlfViSR3TM4fBO4BHkryDBDgrqp6cx3nliSd\noxWDD1BVjwCPLLvt4NTnrwF/Pu5okqQx+UpbSWrC4EtSEwZfkpow+JLUhMGXpCYMviQ1YfAlqQmD\nL0lNGHxJasLgS1ITBl+SmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITB\nl6QmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpow+JLUhMGXpCYMviQ1YfAlqQmDL0lNGHxJasLg\nS1ITg4KfZG+SF5LMJ7n7LGuuT/JkkhNJfjHumJKktdq+0oIk24D7gD8DTgHHkhytquem1lwE3A/s\nraqXk3xivQaWJK3OkCv8a4D5qjpZVe8CR4D9y9bcCjxcVS8DVNUb444pSVqrIcHfAbwydXxqctu0\ny4GLk/w8yfEkt53pGyU5kGQuydzp06dXN7EkaVXGetJ2O3A18BfADcDfJLl8+aKqOlRVs1U1OzMz\nM9JdS5KGWPExfOBV4NKp452T26adAt6qqneAd5I8BlwJvDjKlJKkNRtyhX8M2J1kV5ILgJuBo8vW\n/Bi4Lsn2JB8GvgA8P+6okqS1WPEKv6oWktwJPApsAw5X1Ykkd0zOH6yq55P8DHgaeA94sKqeXc/B\nJUnnJlW1KXc8Oztbc3Nzm3LfkvRBleR4Vc2u5mt9pa0kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpow\n+JLUhMGXpCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0Y\nfElqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpow+JLUhMGXpCYM\nviQ1YfAlqQmDL0lNDAp+kr1JXkgyn+Tu91n3+SQLSW4ab0RJ0hhWDH6SbcB9wD5gD3BLkj1nWfcd\n4B/HHlKStHZDrvCvAear6mRVvQscAfafYd03gB8Cb4w4nyRpJEOCvwN4Zer41OS2/5VkB/Bl4IH3\n+0ZJDiSZSzJ3+vTpc51VkrQGYz1p+13grqp67/0WVdWhqpqtqtmZmZmR7lqSNMT2AWteBS6dOt45\nuW3aLHAkCcAlwI1JFqrqR6NMKUlasyHBPwbsTrKLxdDfDNw6vaCqdv3P50keAv7B2EvS+WXF4FfV\nQpI7gUeBbcDhqjqR5I7J+YPrPKMkaQRDrvCpqkeAR5bddsbQV9Vfrn0sSdLYfKWtJDVh8CWpCYMv\nSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpow+JLUhMGX\npCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElqwuBL\nUhMGX5KaMPiS1ITBl6QmDL4kNWHwJamJQcFPsjfJC0nmk9x9hvNfSfJ0kmeSPJ7kyvFHlSStxYrB\nT7INuA/YB+wBbkmyZ9myl4A/qao/AO4BDo09qCRpbYZc4V8DzFfVyap6FzgC7J9eUFWPV9WvJodP\nADvHHVOStFZDgr8DeGXq+NTktrP5GvDTM51IciDJXJK506dPD59SkrRmoz5pm+SLLAb/rjOdr6pD\nVTVbVbMzMzNj3rUkaQXbB6x5Fbh06njn5Lb/I8nngAeBfVX11jjjSZLGMuQK/xiwO8muJBcANwNH\npxckuQx4GPhqVb04/piSpLVa8Qq/qhaS3Ak8CmwDDlfViSR3TM4fBL4FfBy4PwnAQlXNrt/YkqRz\nlaralDuenZ2tubm5TblvSfqgSnJ8tRfUvtJWkpow+JLUhMGXpCYMviQ1YfAlqQmDL0lNGHxJasLg\nS1ITBl+SmjD4ktSEwZekJgy+JDVh8CWpCYMvSU0YfElqwuBLUhMGX5KaMPiS1ITBl6QmDL4kNWHw\nJakJgy9JTRh8SWrC4EtSEwZfkpow+JLUhMGXpCYMviQ1YfAlqQmDL0lNGHxJasLgS1ITBl+SmjD4\nktSEwZekJgYFP8neJC8kmU9y9xnOJ8n3JuefTnLV+KNKktZixeAn2QbcB+wD9gC3JNmzbNk+YPfk\n4wDwwMhzSpLWaMgV/jXAfFWdrKp3gSPA/mVr9gPfr0VPABcl+dTIs0qS1mD7gDU7gFemjk8BXxiw\nZgfw+vSiJAdY/B8AwH8lefacpt26LgHe3OwhzhPuxRL3Yol7seQzq/3CIcEfTVUdAg4BJJmrqtmN\nvP/zlXuxxL1Y4l4scS+WJJlb7dcOeUjnVeDSqeOdk9vOdY0kaRMNCf4xYHeSXUkuAG4Gji5bcxS4\nbfLbOtcCv66q15d/I0nS5lnxIZ2qWkhyJ/AosA04XFUnktwxOX8QeAS4EZgHfgPcPuC+D6166q3H\nvVjiXixxL5a4F0tWvRepqjEHkSSdp3ylrSQ1YfAlqYl1D75vy7BkwF58ZbIHzyR5PMmVmzHnRlhp\nL6bWfT7JQpKbNnK+jTRkL5Jcn+TJJCeS/GKjZ9woA/6OfCzJT5I8NdmLIc8XfuAkOZzkjbO9VmnV\n3ayqdftg8Unefwd+D7gAeArYs2zNjcBPgQDXAr9cz5k262PgXvwRcPHk832d92Jq3T+z+EsBN232\n3Jv4c3ER8Bxw2eT4E5s99ybuxV8D35l8PgO8DVyw2bOvw178MXAV8OxZzq+qm+t9he/bMixZcS+q\n6vGq+tXk8AkWX8+wFQ35uQD4BvBD4I2NHG6DDdmLW4GHq+plgKraqvsxZC8K+GiSAB9hMfgLGzvm\n+quqx1j8s53Nqrq53sE/21sunOuareBc/5xfY/Ff8K1oxb1IsgP4Mlv/jfiG/FxcDlyc5OdJjie5\nbcOm21hD9uJe4LPAa8AzwDer6r2NGe+8sqpubuhbK2iYJF9kMfjXbfYsm+i7wF1V9d7ixVxr24Gr\ngS8Bvw38a5InqurFzR1rU9wAPAn8KfD7wD8l+Zeq+s/NHeuDYb2D79syLBn050zyOeBBYF9VvbVB\ns220IXsxCxyZxP4S4MYkC1X1o40ZccMM2YtTwFtV9Q7wTpLHgCuBrRb8IXtxO/C3tfhA9nySl4Ar\ngH/bmBHPG6vq5no/pOPbMixZcS+SXAY8DHx1i1+9rbgXVbWrqj5dVZ8G/h74qy0Yexj2d+THwHVJ\ntif5MIvvVvv8Bs+5EYbsxcss/k+HJJ9k8Z0jT27olOeHVXVzXa/wa/3eluEDZ+BefAv4OHD/5Mp2\nobbgOwQO3IsWhuxFVT2f5GfA08B7wINVteXeWnzgz8U9wENJnmHxN1Tuqqot97bJSX4AXA9ckuQU\n8G3gQ7C2bvrWCpLUhK+0laQmDL4kNWHwJakJgy9JTRh8SWrC4EtSEwZfkpr4bz3EZ6V9PH3fAAAA\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11889d1d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot outputs\n",
    "plt.scatter(X_test, y_test,  color='black')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4: Ridge and Lasso Regression (25 points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the same boston data from before, in this question you will explore the application of Lasso and Ridge regression using sklearn package in Python. The following code will split the data into training and test set using [train_test_split](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) with **random state 20** and **test_size = 0.33**.  Note: lambda is called alpha in sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.33, random_state=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Use sklearn.linear_model.Lasso and sklearn.linear_model.Ridge classes to do a [5-fold cross validation](http://scikit-learn.org/stable/auto_examples/exercises/plot_cv_diabetes.html#example-exercises-plot-cv-diabetes-py) using sklearn's [KFold](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.KFold.html). For the sweep of the regularization parameter, we will look at a grid of values ranging from $\\lambda = 10^{10}$ to $\\lambda = 10^{-2}$. In Python, you can consider this range of values as follows:\n",
    "\n",
    "      import numpy as np\n",
    "\n",
    "      alphas =  10**np.linspace(10,-2,100)*0.5\n",
    "\n",
    "  Report the best chosen $\\lambda$ based on cross validation. The cross validation should happen on your training data using  average MSE as the scoring metric. (8pts)\n",
    "\n",
    "2) Run ridge and lasso for all of the alphas specified above (on training data), and plot the coefficients learned for each of them - there should be one plot each for lasso and ridge, so a total of two plots; the plots for different features for a method should be on the same plot (e.g. Fig 6.6 of JW). What do you qualitatively observe when value of the regularization parameter is changed? (7pts)\n",
    "\n",
    "3) Run least squares regression, ridge, and lasso on the training data. For ridge and lasso, use only the best regularization parameter. Report the prediction error (MSE) on the test data for each. (5pts)\n",
    "\n",
    "4) Run lasso again with cross validation using [sklearn.linear_model.LassoCV](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html). Set the cross validation parameters as follows:\n",
    "\n",
    "    LassoCV(alphas=None, cv=10, max_iter=10000)\n",
    "\n",
    "Report the best $\\lambda$ based on cross validation. Run lasso on the training data using the best $\\lambda$ and report the coefficeints for 26 variables. What do you observe from these coefficients? (5pts)\n",
    "\n",
    "## Answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5  (5 pts)\n",
    "\n",
    "A regression model that includes \"interaction terms\" (i.e. quadratic terms of the form $x_ix_j$) as predictors in addition to the linear terms is clearly more general than a corresponding model that employs the same independent variables but only uses the linear terms. Outline two situations where the simpler (less general) model would be preferred to the more powerful model that includes interactive terms.\n",
    "\n",
    "## Answer\n",
    "\n",
    "1: If the data were fit perfectly by a linear approxamation adding interaction terms to the model would only increase the variance of the model, with no significant decrease in bias. An example might be years of education as an undergraduate and total amount paid in tuition. \n",
    "\n",
    "2: If the data were extermely noisy, adding more interaction terms might be just 'chasing noise'. An example might be predicting stock market prices. If you fit the data really well with a complicated model you will likley just be chasing the noise of the stock market and wont make good predictions out of sample. A much simplier model would be suffice to adequetly predict stock market prices, like yesterdays stock price. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
